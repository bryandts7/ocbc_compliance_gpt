{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Store Data to Vector Store (OJK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ini cara untuk storing ke Redis, tapi untuk [Load](#load) Document beda-beda untuk tiap data BI, OJK, dan SIKEPO. Jadi buat sendiri function `extract_all_documents_in_directory` nya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import get_config\n",
    "from utils.models import ModelName, get_model\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import ModelName, LLMModelName, EmbeddingModelName, get_model\n",
    "\n",
    "model_name = ModelName.AZURE_OPENAI\n",
    "llm_model, embed_model = get_model(model_name=model_name, config=config, llm_model_name=LLMModelName.GPT_35_TURBO, embedding_model_name=EmbeddingModelName.EMBEDDING_3_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexing (All)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.documents_extractor.documents_extract_ojk import extract_all_documents_in_directory\n",
    "# from utils.documents_split import document_splitter\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# documents_dirs = ['./data/documents1/', './data/documents2/', './data/documents3/']\n",
    "# pickle_path = './data/pickles/'\n",
    "# metadata_path = './data/metadata/files_metadata.csv'\n",
    "\n",
    "# LOAD_PICKLE = False\n",
    "\n",
    "# for dir in documents_dirs:\n",
    "#     if not LOAD_PICKLE:\n",
    "#         documents = extract_all_documents_in_directory(dir, metadata_path, treshold=0.98)\n",
    "\n",
    "#     if not LOAD_PICKLE:\n",
    "#         all_splits = document_splitter(docs=documents)\n",
    "#         all_splits_sorted = sorted(all_splits, key=lambda x: (x.metadata['doc_id'], x.metadata.get('page_number', '0')))\n",
    "\n",
    "#         # Determine the pickle file name based on the directory\n",
    "#         dir_name = os.path.basename(os.path.normpath(dir))\n",
    "#         file_pickle_name = f'{dir_name}.pkl'\n",
    "\n",
    "#         # Save the sorted splits to a pickle file\n",
    "#         with open(os.path.join(pickle_path, file_pickle_name), 'wb') as file:\n",
    "#             pickle.dump(all_splits_sorted, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir = './data/documents1/'\n",
    "pickle_path = './data/pickles/'\n",
    "metadata_path = './data/metadata/files_metadata.csv'\n",
    "\n",
    "LOAD_PICKLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load**\n",
    "\n",
    "Untuk SIKEPO dan BI beda cara extract documentsnya, file document_extractor buat sendiri :D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.documents_extractor.documents_extract_ojk import extract_all_documents_in_directory\n",
    "\n",
    "if not LOAD_PICKLE:\n",
    "    documents = extract_all_documents_in_directory(documents_dir, metadata_path, treshold=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_content(content):\n",
    "    return content.replace('\\x00', '')  # Remove NUL characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.documents_split import document_splitter\n",
    "import pickle\n",
    "\n",
    "if not LOAD_PICKLE:\n",
    "    all_splits = document_splitter(docs=documents)\n",
    "    all_splits1 = sorted(all_splits, key=lambda x: (x.metadata['doc_id'], x.metadata.get('page_number', '0')))\n",
    "    for split in all_splits1:\n",
    "        split.page_content = clean_document_content(split.page_content)\n",
    "    # Open a file and use dump() \n",
    "    with open(pickle_path + 'documents1.pkl', 'wb') as file:\n",
    "        # A new file will be created\n",
    "        pickle.dump(all_splits1, file)\n",
    "\n",
    "# Open the file in binary mode \n",
    "with open(pickle_path + 'documents3.pkl', 'rb') as file:\n",
    "    # Call load method to deserialze \n",
    "    all_splits = pickle.load(file)\n",
    "    for split in all_splits:\n",
    "        split.page_content = clean_document_content(split.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49253"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'vector_store' already exists.\n",
      "Vector extension created successfully (if it didn't exist).\n",
      "Loaded collection 'ojk_collection'.\n"
     ]
    }
   ],
   "source": [
    "from database.vector_store.vector_store import PostgresIndexManager\n",
    "\n",
    "# vector_store_manager = RedisIndexManager(index_name='ojk', embed_model=embed_model, config=config, db_id=0)\n",
    "vector_store_manager = PostgresIndexManager(index_name='ojk', embed_model=embed_model, config=config)\n",
    "\n",
    "# vector_store_manager.delete_index() # WARNING: This will delete the index\n",
    "# vector_store_manager.store_vector_index(docs=all_splits, batch_size=5000)\n",
    "vector_store = vector_store_manager.load_vector_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NYOBA2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Peraturan Menteri Keuangan Nomor 36/PMK.010/2008 tentang Besar Santunan dan Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan', 'doc_id': 1718, 'sector': 'IKNB', 'file_url': 'https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-36PMK.010-Tahun-2008-tentang-Besar-Santunan-dan-Sumbangan-Wajib-Dana-Kecelakaan-Lalu-Lintas-Jalan/menas13_1389258036.pdf', 'subsector': 'Asuransi', 'page_number': 3, 'effective_date': '2008/02/26', 'regulation_type': 'Klasifikasi Bapepam', 'regulation_number': '36/PMK.010/2008'}, page_content='b. Traktor, buidozer, forklift, mobil derek, excavator, crane dan\\nsejenisnya sebesar Rp 20.000,00 (dua puluh ribu rupiah):\\n Sepeda motor, sepeda kumbang dan scooter di atas 50 cc sampai\\n250 cc dan kendaraan bermotor roda tiga sebesar Rp 32.000,00\\n(tiga puluh dua ribu rupiah).\\nd.\\nPick up/mobil barang sampai dengan 2400 cc, sedan, jeep dan\\n mobil penumpang bukan angkutan umum sebesar Rp140.000,00\\n(seratus empat puluh ribu rupiah).\\n\\nSource: [36/PMK.010/2008](https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-36PMK.010-Tahun-2008-tentang-Besar-Santunan-dan-Sumbangan-Wajib-Dana-Kecelakaan-Lalu-Lintas-Jalan/menas13_1389258036.pdf#page=3)'),\n",
       " Document(metadata={'title': 'Peraturan Menteri Keuangan Nomor 01/PMK.010/2011 tentang Perubahan Atas Peraturan Menteri Keuangan Nomor 74/PMK.010/2007 tentang Penyelenggaraan Pertanggungan Asuransi Pada Lini Usaha Asuransi Kendaraan Bermotor', 'doc_id': 1623, 'sector': 'IKNB', 'file_url': 'https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-01PMK.010-Tahun-2011-tentang-Perubahan-Atas-PMK-Nomor-74PMK.010-Tahun-2007/menas6_1389242661.pdf', 'subsector': 'Asuransi', 'page_number': 28, 'effective_date': '2011/01/04', 'regulation_type': 'Peraturan/Keputusan Mentri', 'regulation_number': '01/PMK.010/2011'}, page_content='MENTERI KEUANGAN\\nREPUBLIK INDONESIA\\n SCANIA\\n265\\n SCANIA BUS\\n266\\n66099\\nLainnya\\n79001\\nLainnya\\nLainnya\\n80001\\n268\\nDELTA\\n DAIHATSU\\n269\\nDAIHATSU\\nZebra Pick Up\\n270\\n80099\\nDAIHATSU\\nLainnya\\n271\\n81\\n272\\n81\\n002\\n273\\n81\\n003\\nSeri FM\\nSeri SG\\n81\\n004\\nHINO\\n81005\\n275\\nDUTRO\\n099\\nHINO\\n276\\n81\\nLainnya\\n277\\n001\\n82\\n002\\n278\\n82\\n ISUZU\\n279\\nISUZU\\n82003\\nISUZU\\n82\\n099\\n83001\\nFUSO\\n282\\n002\\n83003\\n283\\nTRONTON\\n83004\\n284\\nMITSUBISHI\\nKuda Pick Up\\nLainnya\\n285\\n83\\nMITSUBISHI\\n84\\nNISSAN\\nCDA\\n286\\n287\\nCKA\\n84003\\nNISSAN\\n288\\nCWA\\n84004\\nNISSAN\\nPKC\\n289\\nNISSAN\\nPKD\\n84005\\n291\\n84099\\nNISSAN\\nTOYOTA\\n292\\nDYNA RINO\\n85\\n001\\n293\\n85002\\nTOYOTA\\nDYNA 115 S\\n294\\n85003\\n295\\nTOYOTA\\n85.\\nLainnya\\nSCANIA\\nTRONTON\\n86\\n297\\n SCANIA\\nLainnya\\n86\\n298\\n99\\n001\\n Lainnya\\nMENTERI KEUANGAN\\n Salinan sesuai denganaslinya\\nttd.\\nKEPALA/B\\nAGUS D.W. MARTOWARDOJO\\nGIARTO\\n\\nSource: [01/PMK.010/2011](https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-01PMK.010-Tahun-2011-tentang-Perubahan-Atas-PMK-Nomor-74PMK.010-Tahun-2007/menas6_1389242661.pdf#page=28)'),\n",
       " Document(metadata={'title': 'SEOJK tentang Pelaporan Data Risiko Asuransi', 'doc_id': 1339, 'sector': 'IKNB', 'file_url': 'https://www.ojk.go.id/id/regulasi/Documents/Pages/SEOJK-tentang-Pelaporan-Data-Risiko-Asuransi/seojk%2028-2015.pdf', 'subsector': 'Asuransi', 'page_number': 44, 'effective_date': '2015/09/28', 'regulation_type': 'Surat Edaran OJK', 'regulation_number': '28/SEOJK.05/2015'}, page_content='TOYOTA \\nLainnya \\n411 \\n66 001 \\nSCANIA \\nSCANIA BUS \\n412 \\n66 099 \\nSCANIA \\nLainnya \\n413 \\n67 001 \\nMERCEDEZ BENZ \\nOH 1518  \\n414 \\n67 002 \\nMERCEDEZ BENZ \\nOH 1521 \\n415 \\n67 003 \\nMERCEDEZ BENZ \\nOH 1525 \\n416 \\n67 004 \\nMERCEDEZ BENZ \\nOH 1526 \\n417 \\n67 005 \\nMERCEDEZ BENZ \\nOH 1626  \\n418 \\n67 006 \\nMERCEDEZ BENZ \\nOH 1836 \\n419 \\n67 099 \\nMERCEDEZ BENZ \\nLainnya \\n420 \\n79 001 \\nLainnya \\nLainnya \\n  \\nKENDARAAN ANGKUTAN BARANG (TRUK, PICK UP) \\n421 \\n80 001 \\nDAIHATSU \\nDELTA  \\n422 \\n80 002 \\nDAIHATSU \\nZebra Pick Up \\n423 \\n80 003 \\nDAIHATSU \\nGRAN MAX PICK UP \\n424 \\n80 099 \\nDAIHATSU \\nLainnya \\n425 \\n81 001 \\nHINO \\nSeri FF  \\n426 \\n81 002 \\nHINO \\nSeri FL  \\n427 \\n81 003 \\nHINO \\nSeri FM  \\n428 \\n81 004 \\nHINO \\nSeri SG  \\n429 \\n81 005 \\nHINO \\nDUTRO  \\n430 \\n81 006 \\nHINO \\n110 \\n431 \\n81 007 \\nHINO \\n130 \\n432 \\n81 099 \\nHINO \\nLainnya \\n433 \\n82 001 \\nISUZU \\nBORNEO  \\n434 \\n82 002 \\nISUZU \\nCXZ  \\n435 \\n82 003 \\nISUZU \\nELF \\n436 \\n82 004 \\nISUZU \\nBISON\\n\\nSource: [28/SEOJK.05/2015](https://www.ojk.go.id/id/regulasi/Documents/Pages/SEOJK-tentang-Pelaporan-Data-Risiko-Asuransi/seojk%2028-2015.pdf#page=44)'),\n",
       " Document(metadata={'title': 'Peraturan Menteri Keuangan Nomor 36/PMK.010/2008 tentang Besar Santunan dan Sumbangan Wajib Dana Kecelakaan Lalu Lintas Jalan', 'doc_id': 1718, 'sector': 'IKNB', 'file_url': 'https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-36PMK.010-Tahun-2008-tentang-Besar-Santunan-dan-Sumbangan-Wajib-Dana-Kecelakaan-Lalu-Lintas-Jalan/menas13_1389258036.pdf', 'subsector': 'Asuransi', 'page_number': 4, 'effective_date': '2008/02/26', 'regulation_type': 'Klasifikasi Bapepam', 'regulation_number': '36/PMK.010/2008'}, page_content='Pasal 6\\nTanda Nomor Kendaraan Bermotor sesuai dengan ketentuan\\nperaturan perundang-undangan yang berlaku.\\n2) Dalam hal pembayaran SWDKLLJ dilakukan setelah melewati batas\\n seharusnya dibayar dengan ketentuan denda yang dikenakan paling\\nbesar Rp100.000,00 (seratus ribu rupiah).\\n(3) Dalam hal ketentuan mengenai batas waktu sebagaimana dimaksud\\ngeografis daerah setempat, Direksi perusahaan yang ditunjuk untuk\\nmenyelenggarakan Dana Kecelakaan Lalu Lintas Jalan diberi\\nbesarnya denda SWDKLLJ, dengan ketentuan batas waktu dimaksud\\npaling lama 15 (lima belas) hari kerja.\\n\\nSource: [36/PMK.010/2008](https://www.ojk.go.id/id/regulasi/Documents/Pages/PMK-Nomor-36PMK.010-Tahun-2008-tentang-Besar-Santunan-dan-Sumbangan-Wajib-Dana-Kecelakaan-Lalu-Lintas-Jalan/menas13_1389258036.pdf#page=4)')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector_store.as_retriever().invoke(\"Berapa SWDKLLJ dari buldozer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chains.query_constructor.base import (\n",
    "    StructuredQueryOutputParser,\n",
    "    get_query_constructor_prompt,\n",
    ")\n",
    "from langchain_core.language_models.base import BaseLanguageModel\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain.retrievers.self_query.pgvector import PGVectorTranslator\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.structured_query import Operator, Comparator\n",
    "\n",
    "# Define metadata field information\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title of the document of regulation\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"sector\",\n",
    "        description=\"\"\"The sector of the regulation\"\"\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"subsector\",\n",
    "        description=\"The subsector of the regulation\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"regulation_type\",\n",
    "        description=\"\"\"The type of the regulation\"\"\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"regulation_number\",\n",
    "        description=\"The number of the regulation\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"effective_date\",\n",
    "        description=\"The effective date of the regulation in string format 'YYYY/MM/DD'\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Define document content description\n",
    "document_content_description = \"The content of the document\"\n",
    "\n",
    "# Define prompt\n",
    "SCHEMA = \"\"\"\\\n",
    "<< Structured Request Schema >>\n",
    "When responding use a markdown code snippet with a JSON object formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{{{\n",
    "    \"query\": string \\\\ text string to compare to document contents\n",
    "    \"filter\": string \\\\ logical condition statement for filtering documents\n",
    "}}}}\n",
    "```\n",
    "\n",
    "The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\n",
    "\n",
    "A logical condition statement is composed of one or more comparison and logical operation statements.\n",
    "\n",
    "A comparison statement takes the form: `comp(attr, val)`:\n",
    "- `comp` ({allowed_comparators}): comparator\n",
    "- `attr` (string):  name of attribute to apply the comparison to\n",
    "- `val` (string): is the comparison value\n",
    "\n",
    "A logical operation statement takes the form `op(statement1, statement2, ...)`:\n",
    "- `op` ({allowed_operators}): logical operator\n",
    "- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\n",
    "\n",
    "Make sure that you only use the comparators and logical operators listed above and no others.\n",
    "Make sure that filters only refer to attributes that exist in the data source.\n",
    "Make sure that filters only use the attributed names with its function names if there are functions applied on them.\n",
    "Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\n",
    "Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.\n",
    "Make sure that date attributes are compared using ASCII comparison operators.\n",
    "\"\"\"\n",
    "\n",
    "SCHEMA_PROMPT = PromptTemplate.from_template(SCHEMA)\n",
    "\n",
    "# prompt = get_query_constructor_prompt(\n",
    "#     document_contents=document_content_description,\n",
    "#     attribute_info=metadata_field_info,\n",
    "#     schema_prompt=SCHEMA_PROMPT,\n",
    "    \n",
    "# )\n",
    "# output_parser = StructuredQueryOutputParser.from_components()\n",
    "# query_constructor = prompt | llm_model | output_parser\n",
    "\n",
    "# query_constructor.invoke(\"Berikan dokumen yang berlaku pada tanggal 1 Januari 2023 hingga 1 Januari 2024\")\n",
    "\n",
    "\n",
    "\n",
    "# # Create query constructor\n",
    "# def self_query_ojk(llm_model: BaseLanguageModel, vector_store: VectorStore, search_type: str = \"similarity\") -> SelfQueryRetriever:\n",
    "#     retriever = SelfQueryRetriever.from_llm(\n",
    "#         document_contents=document_content_description,\n",
    "#         # enable_limit=False,\n",
    "#         use_original_query=True,\n",
    "#         llm=llm_model,\n",
    "#         vectorstore=vector_store,\n",
    "#         metadata_field_info=metadata_field_info,\n",
    "#         structured_query_translator=PGVectorTranslator(),\n",
    "#     )\n",
    "\n",
    "#     return retriever\n",
    "\n",
    "\n",
    "\n",
    "def self_query_ojk(llm_model: BaseLanguageModel, vector_store: VectorStore, search_type: str = \"similarity\") -> SelfQueryRetriever:\n",
    "    prompt = get_query_constructor_prompt(\n",
    "        document_contents=document_content_description,\n",
    "        attribute_info=metadata_field_info,\n",
    "        schema_prompt=SCHEMA_PROMPT,\n",
    "        allowed_operators = [Operator.AND, Operator.OR],\n",
    "        # \"\"\"Subset of allowed logical operators.\"\"\"\n",
    "        allowed_comparators = [\n",
    "            Comparator.EQ,\n",
    "            Comparator.NE,\n",
    "            Comparator.GT,\n",
    "            Comparator.LT,\n",
    "            Comparator.IN,\n",
    "            Comparator.NIN,\n",
    "            Comparator.CONTAIN,\n",
    "            Comparator.LIKE,\n",
    "        ]\n",
    "    )\n",
    "    output_parser = StructuredQueryOutputParser.from_components()\n",
    "    query_constructor = prompt | llm_model | output_parser\n",
    "\n",
    "    retriever = SelfQueryRetriever(\n",
    "        query_constructor=query_constructor,\n",
    "        vectorstore=vector_store,\n",
    "        search_type=search_type,\n",
    "        structured_query_translator=PGVectorTranslator(),\n",
    "        verbose=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "retriever = self_query_ojk(llm_model=llm_model, vector_store=vector_store, search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Berikan dokumen pada subsektor Asuransi dan tanggal berlaku pada 1992/02/11\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Berikan dokumen pada subsektor Asuransi dan tanggal berlaku pada 1992/02/11\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > prompt:FewShotPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > llm:AzureChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | lt | in | nin | contain | like): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\nMake sure that date attributes are compared using ASCII comparison operators.\\n\\n\\n<< Example 1. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"teenager love\\\",\\n    \\\"filter\\\": \\\"and(or(eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Taylor Swift\\\\\\\"), eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Katy Perry\\\\\\\")), lt(\\\\\\\"length\\\\\\\", 180), eq(\\\\\\\"genre\\\\\\\", \\\\\\\"pop\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs that were not published on Spotify\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"The content of the document\\\",\\n    \\\"attributes\\\": {\\n    \\\"title\\\": {\\n        \\\"description\\\": \\\"The title of the document of regulation\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"sector\\\": {\\n        \\\"description\\\": \\\"The sector of the regulation\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"subsector\\\": {\\n        \\\"description\\\": \\\"The subsector of the regulation\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"regulation_type\\\": {\\n        \\\"description\\\": \\\"The type of the regulation\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"regulation_number\\\": {\\n        \\\"description\\\": \\\"The number of the regulation\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"effective_date\\\": {\\n        \\\"description\\\": \\\"The effective date of the regulation in string format 'YYYY/MM/DD'\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    }\\n}\\n}\\n```\\n\\nUser Query:\\nBerikan dokumen pada subsektor Asuransi dan tanggal berlaku pada 1992/02/11\\n\\nStructured Request:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > llm:AzureChatOpenAI] [1.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"subsector\\\\\\\", \\\\\\\"Asuransi\\\\\\\"), eq(\\\\\\\"effective_date\\\\\\\", \\\\\\\"1992/02/11\\\\\\\"))\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null,\n",
      "          \"content_filter_results\": {\n",
      "            \"hate\": {\n",
      "              \"filtered\": false,\n",
      "              \"severity\": \"safe\"\n",
      "            },\n",
      "            \"self_harm\": {\n",
      "              \"filtered\": false,\n",
      "              \"severity\": \"safe\"\n",
      "            },\n",
      "            \"sexual\": {\n",
      "              \"filtered\": false,\n",
      "              \"severity\": \"safe\"\n",
      "            },\n",
      "            \"violence\": {\n",
      "              \"filtered\": false,\n",
      "              \"severity\": \"safe\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"subsector\\\\\\\", \\\\\\\"Asuransi\\\\\\\"), eq(\\\\\\\"effective_date\\\\\\\", \\\\\\\"1992/02/11\\\\\\\"))\\\"\\n}\\n```\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 42,\n",
      "                \"prompt_tokens\": 971,\n",
      "                \"total_tokens\": 1013\n",
      "              },\n",
      "              \"model_name\": \"gpt-35-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"prompt_filter_results\": [\n",
      "                {\n",
      "                  \"prompt_index\": 0,\n",
      "                  \"content_filter_results\": {\n",
      "                    \"hate\": {\n",
      "                      \"filtered\": false,\n",
      "                      \"severity\": \"safe\"\n",
      "                    },\n",
      "                    \"self_harm\": {\n",
      "                      \"filtered\": false,\n",
      "                      \"severity\": \"safe\"\n",
      "                    },\n",
      "                    \"sexual\": {\n",
      "                      \"filtered\": false,\n",
      "                      \"severity\": \"safe\"\n",
      "                    },\n",
      "                    \"violence\": {\n",
      "                      \"filtered\": false,\n",
      "                      \"severity\": \"safe\"\n",
      "                    }\n",
      "                  }\n",
      "                }\n",
      "              ],\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null,\n",
      "              \"content_filter_results\": {\n",
      "                \"hate\": {\n",
      "                  \"filtered\": false,\n",
      "                  \"severity\": \"safe\"\n",
      "                },\n",
      "                \"self_harm\": {\n",
      "                  \"filtered\": false,\n",
      "                  \"severity\": \"safe\"\n",
      "                },\n",
      "                \"sexual\": {\n",
      "                  \"filtered\": false,\n",
      "                  \"severity\": \"safe\"\n",
      "                },\n",
      "                \"violence\": {\n",
      "                  \"filtered\": false,\n",
      "                  \"severity\": \"safe\"\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f87dbf0a-0c15-4c35-b55e-90eef46be307-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 971,\n",
      "              \"output_tokens\": 42,\n",
      "              \"total_tokens\": 1013\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 42,\n",
      "      \"prompt_tokens\": 971,\n",
      "      \"total_tokens\": 1013\n",
      "    },\n",
      "    \"model_name\": \"gpt-35-turbo\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"prompt_filter_results\": [\n",
      "      {\n",
      "        \"prompt_index\": 0,\n",
      "        \"content_filter_results\": {\n",
      "          \"hate\": {\n",
      "            \"filtered\": false,\n",
      "            \"severity\": \"safe\"\n",
      "          },\n",
      "          \"self_harm\": {\n",
      "            \"filtered\": false,\n",
      "            \"severity\": \"safe\"\n",
      "          },\n",
      "          \"sexual\": {\n",
      "            \"filtered\": false,\n",
      "            \"severity\": \"safe\"\n",
      "          },\n",
      "          \"violence\": {\n",
      "            \"filtered\": false,\n",
      "            \"severity\": \"safe\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > parser:StructuredQueryOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence > parser:StructuredQueryOutputParser] [2ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:RunnableSequence] [1.54s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2024-07-01\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "context = retriever.invoke('Berikan dokumen pada subsektor Asuransi dan tanggal berlaku pada 1992/02/11')\n",
    "context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
