{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Store Data to Vector Store (OJK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ini cara untuk storing ke Redis, tapi untuk [Load](#load) Document beda-beda untuk tiap data BI, OJK, dan SIKEPO. Jadi buat sendiri function `extract_all_documents_in_directory` nya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import get_config\n",
    "from utils.models import ModelName, get_model\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import ModelName, LLMModelName, EmbeddingModelName, get_model\n",
    "\n",
    "model_name = ModelName.AZURE_OPENAI\n",
    "llm_model, embed_model = get_model(model_name=model_name, config=config, llm_model_name=LLMModelName.GPT_35_TURBO, embedding_model_name=EmbeddingModelName.EMBEDDING_3_SMALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir = './data/documents3/'\n",
    "pickle_path = './data/pickles/'\n",
    "metadata_path = './data/metadata/files_metadata.csv'\n",
    "\n",
    "LOAD_PICKLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load**\n",
    "\n",
    "Untuk SIKEPO dan BI beda cara extract documentsnya, file document_extractor buat sendiri :D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.documents_extractor.documents_extract_ojk import extract_all_documents_in_directory\n",
    "\n",
    "if not LOAD_PICKLE:\n",
    "    documents = extract_all_documents_in_directory(documents_dir, metadata_path, treshold=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.documents_split import document_splitter\n",
    "import pickle\n",
    "\n",
    "\n",
    "if not LOAD_PICKLE:\n",
    "    all_splits = document_splitter(docs=documents)\n",
    "    all_splits1 = sorted(all_splits, key=lambda x: (x.metadata['doc_id'], x.metadata.get('page_number', '0')))\n",
    "    # Open a file and use dump() \n",
    "    with open(pickle_path + 'documents3.pkl', 'wb') as file:\n",
    "\n",
    "        # A new file will be created\n",
    "        pickle.dump(all_splits1, file) \n",
    "\n",
    "# Open the file in binary mode \n",
    "with open(pickle_path + 'documents3.pkl', 'rb') as file:\n",
    "    \n",
    "    # Call load method to deserialze \n",
    "    all_splits = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49253"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading from idx: 0\n",
      "Loaded 1-100 documents\n",
      "Loaded 101-200 documents\n",
      "Loaded 201-300 documents\n",
      "Loaded 301-400 documents\n",
      "Loaded 401-500 documents\n",
      "Loaded 501-600 documents\n",
      "Loaded 601-700 documents\n",
      "Loaded 701-800 documents\n",
      "Loaded 801-900 documents\n",
      "Loaded 901-1000 documents\n"
     ]
    }
   ],
   "source": [
    "from database.vector_store import RedisIndexManager\n",
    "\n",
    "redis = RedisIndexManager(index_name='ojk', embed_model=embed_model, config=config, db_id=0)\n",
    "\n",
    "# redis.delete_index()\n",
    "redis.store_vector_index(docs=all_splits, batch_size=100) # Kalau error 'Redis failed to connect: Index does not exist.' ubah isi start_store_idx_indexname.txt menjadi 0\n",
    "vector_store = redis.load_vector_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
